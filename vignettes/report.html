<!DOCTYPE html>

<html xmlns="http://www.w3.org/1999/xhtml">

<head>

<meta charset="utf-8" />
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="generator" content="pandoc" />

<meta name="viewport" content="width=device-width, initial-scale=1">

<meta name="author" content="Sixian Zhang, Zaoyi Chi, Hao Wang, Zhaolu Yang" />

<meta name="date" content="2019-03-08" />

<title>Report for Coding project 2: linear models for regression and binary classification</title>



<style type="text/css">code{white-space: pre;}</style>
<style type="text/css" data-origin="pandoc">
div.sourceCode { overflow-x: auto; }
table.sourceCode, tr.sourceCode, td.lineNumbers, td.sourceCode {
  margin: 0; padding: 0; vertical-align: baseline; border: none; }
table.sourceCode { width: 100%; line-height: 100%; }
td.lineNumbers { text-align: right; padding-right: 4px; padding-left: 4px; color: #aaaaaa; border-right: 1px solid #aaaaaa; }
td.sourceCode { padding-left: 5px; }
code > span.kw { color: #007020; font-weight: bold; } /* Keyword */
code > span.dt { color: #902000; } /* DataType */
code > span.dv { color: #40a070; } /* DecVal */
code > span.bn { color: #40a070; } /* BaseN */
code > span.fl { color: #40a070; } /* Float */
code > span.ch { color: #4070a0; } /* Char */
code > span.st { color: #4070a0; } /* String */
code > span.co { color: #60a0b0; font-style: italic; } /* Comment */
code > span.ot { color: #007020; } /* Other */
code > span.al { color: #ff0000; font-weight: bold; } /* Alert */
code > span.fu { color: #06287e; } /* Function */
code > span.er { color: #ff0000; font-weight: bold; } /* Error */
code > span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
code > span.cn { color: #880000; } /* Constant */
code > span.sc { color: #4070a0; } /* SpecialChar */
code > span.vs { color: #4070a0; } /* VerbatimString */
code > span.ss { color: #bb6688; } /* SpecialString */
code > span.im { } /* Import */
code > span.va { color: #19177c; } /* Variable */
code > span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code > span.op { color: #666666; } /* Operator */
code > span.bu { } /* BuiltIn */
code > span.ex { } /* Extension */
code > span.pp { color: #bc7a00; } /* Preprocessor */
code > span.at { color: #7d9029; } /* Attribute */
code > span.do { color: #ba2121; font-style: italic; } /* Documentation */
code > span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code > span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code > span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */

</style>
<script>
// apply pandoc div.sourceCode style to pre.sourceCode instead
(function() {
  var sheets = document.styleSheets;
  for (var i = 0; i < sheets.length; i++) {
    if (sheets[i].ownerNode.dataset["origin"] !== "pandoc") continue;
    try { var rules = sheets[i].cssRules; } catch (e) { continue; }
    for (var j = 0; j < rules.length; j++) {
      var rule = rules[j];
      // check if there is a div.sourceCode rule
      if (rule.type !== rule.STYLE_RULE || rule.selectorText !== "div.sourceCode") continue;
      var style = rule.style.cssText;
      // check if color or background-color is set
      if (rule.style.color === '' || rule.style.backgroundColor === '') continue;
      // replace div.sourceCode by a pre.sourceCode rule
      sheets[i].deleteRule(j);
      sheets[i].insertRule('pre.sourceCode{' + style + '}', j);
    }
  }
})();
</script>



<link href="data:text/css;charset=utf-8,body%20%7B%0Abackground%2Dcolor%3A%20%23fff%3B%0Amargin%3A%201em%20auto%3B%0Amax%2Dwidth%3A%20700px%3B%0Aoverflow%3A%20visible%3B%0Apadding%2Dleft%3A%202em%3B%0Apadding%2Dright%3A%202em%3B%0Afont%2Dfamily%3A%20%22Open%20Sans%22%2C%20%22Helvetica%20Neue%22%2C%20Helvetica%2C%20Arial%2C%20sans%2Dserif%3B%0Afont%2Dsize%3A%2014px%3B%0Aline%2Dheight%3A%201%2E35%3B%0A%7D%0A%23header%20%7B%0Atext%2Dalign%3A%20center%3B%0A%7D%0A%23TOC%20%7B%0Aclear%3A%20both%3B%0Amargin%3A%200%200%2010px%2010px%3B%0Apadding%3A%204px%3B%0Awidth%3A%20400px%3B%0Aborder%3A%201px%20solid%20%23CCCCCC%3B%0Aborder%2Dradius%3A%205px%3B%0Abackground%2Dcolor%3A%20%23f6f6f6%3B%0Afont%2Dsize%3A%2013px%3B%0Aline%2Dheight%3A%201%2E3%3B%0A%7D%0A%23TOC%20%2Etoctitle%20%7B%0Afont%2Dweight%3A%20bold%3B%0Afont%2Dsize%3A%2015px%3B%0Amargin%2Dleft%3A%205px%3B%0A%7D%0A%23TOC%20ul%20%7B%0Apadding%2Dleft%3A%2040px%3B%0Amargin%2Dleft%3A%20%2D1%2E5em%3B%0Amargin%2Dtop%3A%205px%3B%0Amargin%2Dbottom%3A%205px%3B%0A%7D%0A%23TOC%20ul%20ul%20%7B%0Amargin%2Dleft%3A%20%2D2em%3B%0A%7D%0A%23TOC%20li%20%7B%0Aline%2Dheight%3A%2016px%3B%0A%7D%0Atable%20%7B%0Amargin%3A%201em%20auto%3B%0Aborder%2Dwidth%3A%201px%3B%0Aborder%2Dcolor%3A%20%23DDDDDD%3B%0Aborder%2Dstyle%3A%20outset%3B%0Aborder%2Dcollapse%3A%20collapse%3B%0A%7D%0Atable%20th%20%7B%0Aborder%2Dwidth%3A%202px%3B%0Apadding%3A%205px%3B%0Aborder%2Dstyle%3A%20inset%3B%0A%7D%0Atable%20td%20%7B%0Aborder%2Dwidth%3A%201px%3B%0Aborder%2Dstyle%3A%20inset%3B%0Aline%2Dheight%3A%2018px%3B%0Apadding%3A%205px%205px%3B%0A%7D%0Atable%2C%20table%20th%2C%20table%20td%20%7B%0Aborder%2Dleft%2Dstyle%3A%20none%3B%0Aborder%2Dright%2Dstyle%3A%20none%3B%0A%7D%0Atable%20thead%2C%20table%20tr%2Eeven%20%7B%0Abackground%2Dcolor%3A%20%23f7f7f7%3B%0A%7D%0Ap%20%7B%0Amargin%3A%200%2E5em%200%3B%0A%7D%0Ablockquote%20%7B%0Abackground%2Dcolor%3A%20%23f6f6f6%3B%0Apadding%3A%200%2E25em%200%2E75em%3B%0A%7D%0Ahr%20%7B%0Aborder%2Dstyle%3A%20solid%3B%0Aborder%3A%20none%3B%0Aborder%2Dtop%3A%201px%20solid%20%23777%3B%0Amargin%3A%2028px%200%3B%0A%7D%0Adl%20%7B%0Amargin%2Dleft%3A%200%3B%0A%7D%0Adl%20dd%20%7B%0Amargin%2Dbottom%3A%2013px%3B%0Amargin%2Dleft%3A%2013px%3B%0A%7D%0Adl%20dt%20%7B%0Afont%2Dweight%3A%20bold%3B%0A%7D%0Aul%20%7B%0Amargin%2Dtop%3A%200%3B%0A%7D%0Aul%20li%20%7B%0Alist%2Dstyle%3A%20circle%20outside%3B%0A%7D%0Aul%20ul%20%7B%0Amargin%2Dbottom%3A%200%3B%0A%7D%0Apre%2C%20code%20%7B%0Abackground%2Dcolor%3A%20%23f7f7f7%3B%0Aborder%2Dradius%3A%203px%3B%0Acolor%3A%20%23333%3B%0Awhite%2Dspace%3A%20pre%2Dwrap%3B%20%0A%7D%0Apre%20%7B%0Aborder%2Dradius%3A%203px%3B%0Amargin%3A%205px%200px%2010px%200px%3B%0Apadding%3A%2010px%3B%0A%7D%0Apre%3Anot%28%5Bclass%5D%29%20%7B%0Abackground%2Dcolor%3A%20%23f7f7f7%3B%0A%7D%0Acode%20%7B%0Afont%2Dfamily%3A%20Consolas%2C%20Monaco%2C%20%27Courier%20New%27%2C%20monospace%3B%0Afont%2Dsize%3A%2085%25%3B%0A%7D%0Ap%20%3E%20code%2C%20li%20%3E%20code%20%7B%0Apadding%3A%202px%200px%3B%0A%7D%0Adiv%2Efigure%20%7B%0Atext%2Dalign%3A%20center%3B%0A%7D%0Aimg%20%7B%0Abackground%2Dcolor%3A%20%23FFFFFF%3B%0Apadding%3A%202px%3B%0Aborder%3A%201px%20solid%20%23DDDDDD%3B%0Aborder%2Dradius%3A%203px%3B%0Aborder%3A%201px%20solid%20%23CCCCCC%3B%0Amargin%3A%200%205px%3B%0A%7D%0Ah1%20%7B%0Amargin%2Dtop%3A%200%3B%0Afont%2Dsize%3A%2035px%3B%0Aline%2Dheight%3A%2040px%3B%0A%7D%0Ah2%20%7B%0Aborder%2Dbottom%3A%204px%20solid%20%23f7f7f7%3B%0Apadding%2Dtop%3A%2010px%3B%0Apadding%2Dbottom%3A%202px%3B%0Afont%2Dsize%3A%20145%25%3B%0A%7D%0Ah3%20%7B%0Aborder%2Dbottom%3A%202px%20solid%20%23f7f7f7%3B%0Apadding%2Dtop%3A%2010px%3B%0Afont%2Dsize%3A%20120%25%3B%0A%7D%0Ah4%20%7B%0Aborder%2Dbottom%3A%201px%20solid%20%23f7f7f7%3B%0Amargin%2Dleft%3A%208px%3B%0Afont%2Dsize%3A%20105%25%3B%0A%7D%0Ah5%2C%20h6%20%7B%0Aborder%2Dbottom%3A%201px%20solid%20%23ccc%3B%0Afont%2Dsize%3A%20105%25%3B%0A%7D%0Aa%20%7B%0Acolor%3A%20%230033dd%3B%0Atext%2Ddecoration%3A%20none%3B%0A%7D%0Aa%3Ahover%20%7B%0Acolor%3A%20%236666ff%3B%20%7D%0Aa%3Avisited%20%7B%0Acolor%3A%20%23800080%3B%20%7D%0Aa%3Avisited%3Ahover%20%7B%0Acolor%3A%20%23BB00BB%3B%20%7D%0Aa%5Bhref%5E%3D%22http%3A%22%5D%20%7B%0Atext%2Ddecoration%3A%20underline%3B%20%7D%0Aa%5Bhref%5E%3D%22https%3A%22%5D%20%7B%0Atext%2Ddecoration%3A%20underline%3B%20%7D%0A%0Acode%20%3E%20span%2Ekw%20%7B%20color%3A%20%23555%3B%20font%2Dweight%3A%20bold%3B%20%7D%20%0Acode%20%3E%20span%2Edt%20%7B%20color%3A%20%23902000%3B%20%7D%20%0Acode%20%3E%20span%2Edv%20%7B%20color%3A%20%2340a070%3B%20%7D%20%0Acode%20%3E%20span%2Ebn%20%7B%20color%3A%20%23d14%3B%20%7D%20%0Acode%20%3E%20span%2Efl%20%7B%20color%3A%20%23d14%3B%20%7D%20%0Acode%20%3E%20span%2Ech%20%7B%20color%3A%20%23d14%3B%20%7D%20%0Acode%20%3E%20span%2Est%20%7B%20color%3A%20%23d14%3B%20%7D%20%0Acode%20%3E%20span%2Eco%20%7B%20color%3A%20%23888888%3B%20font%2Dstyle%3A%20italic%3B%20%7D%20%0Acode%20%3E%20span%2Eot%20%7B%20color%3A%20%23007020%3B%20%7D%20%0Acode%20%3E%20span%2Eal%20%7B%20color%3A%20%23ff0000%3B%20font%2Dweight%3A%20bold%3B%20%7D%20%0Acode%20%3E%20span%2Efu%20%7B%20color%3A%20%23900%3B%20font%2Dweight%3A%20bold%3B%20%7D%20%20code%20%3E%20span%2Eer%20%7B%20color%3A%20%23a61717%3B%20background%2Dcolor%3A%20%23e3d2d2%3B%20%7D%20%0A" rel="stylesheet" type="text/css" />

</head>

<body>




<h1 class="title toc-ignore">Report for Coding project 2: linear models for regression and binary classification</h1>
<h4 class="author"><em>Sixian Zhang, Zaoyi Chi, Hao Wang, Zhaolu Yang</em></h4>
<h4 class="date"><em>2019-03-08</em></h4>



<div id="introduction" class="section level2">
<h2>Introduction</h2>
<p>For this project we created an R package with R code that implements a version of the linear models for both regression and binary classification.</p>
<p>Here are some significant formulas that have been used in this function:</p>
<p><strong>Loss Function: </strong></p>
<p><strong>1.Square Loss (Regression): </strong><span class="math inline">\(L(w) = ||Xw-y||^2_2\)</span></p>
<p><strong>2.Logistic Loss (Binary classification): </strong><span class="math inline">\(L(w) = \sum^{n}_{i=1}log[1+exp(-\hat{y_i}w^tx)]\)</span></p>
<p><strong>Cost Function: </strong> <span class="math inline">\(C_\lambda(w) = L(w)+\lambda||w||^2_2\)</span></p>
</div>
<div id="main-function" class="section level2">
<h2>Main Function</h2>
<p>The purpose of this section is to give users a general information of this package. We will briefly go over the main functions.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">## Source Code:
<span class="kw">library</span>(LinearModel)

<span class="kw">data</span>(spam, <span class="dt">package =</span> <span class="st">&quot;ElemStatLearn&quot;</span>)
<span class="kw">data</span>(SAheart, <span class="dt">package =</span> <span class="st">&quot;ElemStatLearn&quot;</span>)
<span class="kw">data</span>(zip.train, <span class="dt">package =</span> <span class="st">&quot;ElemStatLearn&quot;</span>)
zip.train &lt;-<span class="st"> </span>zip.train[zip.train[,<span class="dv">1</span>] <span class="op">%in%</span><span class="st"> </span><span class="kw">c</span>(<span class="dv">0</span>,<span class="dv">1</span>),]
<span class="kw">data</span>(prostate, <span class="dt">package =</span> <span class="st">&quot;ElemStatLearn&quot;</span>)
<span class="kw">data</span>(ozone, <span class="dt">package =</span> <span class="st">&quot;ElemStatLearn&quot;</span>)

data.list &lt;-<span class="st"> </span><span class="kw">list</span>(
  <span class="dt">spam =</span> <span class="kw">list</span>(
    <span class="dt">features =</span> <span class="kw">as.matrix</span>(spam[, <span class="dv">1</span><span class="op">:</span><span class="dv">57</span>]),
    <span class="dt">labels =</span> <span class="kw">ifelse</span>(spam<span class="op">$</span>spam <span class="op">==</span><span class="st"> &quot;spam&quot;</span>, <span class="dv">1</span>, <span class="dv">0</span>),
    <span class="dt">is.01 =</span> <span class="ot">TRUE</span>
  ),

  <span class="dt">SAheart =</span> <span class="kw">list</span>(
    <span class="dt">features =</span> <span class="kw">as.matrix</span>(SAheart[, <span class="kw">c</span>(<span class="dv">1</span><span class="op">:</span><span class="dv">4</span>,<span class="dv">6</span><span class="op">:</span><span class="dv">9</span>)]),
    <span class="dt">labels =</span> SAheart<span class="op">$</span>chd,
    <span class="dt">is.01 =</span> <span class="ot">TRUE</span>
  ),

  <span class="dt">zip.train =</span> <span class="kw">list</span>(
    <span class="dt">features =</span> <span class="kw">as.matrix</span>(zip.train[, <span class="op">-</span><span class="dv">1</span>]),
    <span class="dt">labels =</span> zip.train[, <span class="dv">1</span>],
    <span class="dt">is.01 =</span> <span class="ot">TRUE</span>
  ),

  <span class="dt">prostate =</span> <span class="kw">list</span>(<span class="dt">features =</span> <span class="kw">as.matrix</span>(prostate[, <span class="dv">1</span><span class="op">:</span><span class="dv">8</span>]),
                  <span class="dt">labels =</span> prostate<span class="op">$</span>lpsa,
                  <span class="dt">is.01 =</span> <span class="ot">FALSE</span>),

  <span class="dt">ozone =</span> <span class="kw">list</span>(<span class="dt">features =</span> <span class="kw">as.matrix</span>(ozone[,<span class="op">-</span><span class="dv">1</span>]),
               <span class="dt">labels =</span> ozone[, <span class="dv">1</span>],
               <span class="dt">is.01 =</span> <span class="ot">FALSE</span>)
)

n.folds &lt;-<span class="st"> </span>4L</code></pre></div>
</div>
<div id="experimentsapplication" class="section level2">
<h2>Experiments/application</h2>
<p>We are going to run our code on the following data sets.</p>
</div>
<div id="data-set-1-spam" class="section level2">
<h2>Data set 1: spam</h2>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co">#Spam</span>
data.name  =<span class="st"> </span><span class="dv">1</span>
data.set &lt;-<span class="st"> </span>data.list[[data.name]]
test.loss.mat &lt;-<span class="st"> </span><span class="kw">matrix</span>(<span class="dv">0</span>, <span class="dt">nrow =</span> <span class="dv">4</span>, <span class="dt">ncol =</span> <span class="dv">3</span>)

<span class="co">#Check data type here:</span>
<span class="kw">set.seed</span>(<span class="dv">2</span>)

fold.vec &lt;-<span class="st"> </span><span class="kw">sample</span>(<span class="kw">rep</span>(<span class="dv">1</span><span class="op">:</span>n.folds, <span class="dt">l =</span> <span class="kw">length</span>(data.set<span class="op">$</span>labels)))

penalty.vec &lt;-<span class="st"> </span><span class="kw">seq</span>(<span class="dv">5</span>, <span class="fl">0.1</span>, <span class="dt">by =</span> <span class="op">-</span><span class="fl">0.1</span>)

<span class="cf">for</span> (i.fold <span class="cf">in</span> (<span class="dv">1</span><span class="op">:</span>n.folds)) {
  train.index &lt;-<span class="st"> </span>fold.vec <span class="op">!=</span><span class="st"> </span>i.fold
  test.index &lt;-<span class="st"> </span><span class="op">!</span>train.index

  x.train &lt;-<span class="st"> </span>data.set<span class="op">$</span>features[train.index, ]
  y.train &lt;-<span class="st"> </span>data.set<span class="op">$</span>labels[train.index]
  x.test &lt;-<span class="st"> </span>data.set<span class="op">$</span>feature[test.index, ]
  y.test &lt;-<span class="st"> </span>data.set<span class="op">$</span>labels[test.index]

  <span class="cf">if</span> (data.set<span class="op">$</span>is.<span class="dv">01</span>) {
    <span class="co"># binary data</span>
    earlystopping.list &lt;-
<span class="st">      </span><span class="kw">LMLogisticLossEarlyStoppingCV</span>(x.train, y.train, <span class="ot">NULL</span>, 100L, <span class="fl">0.5</span>)
    L2.list &lt;-
<span class="st">      </span><span class="kw">LMLogisticLossL2CV</span>(x.train, y.train, <span class="ot">NULL</span>, penalty.vec)

    earlystopping.predict &lt;-
<span class="st">      </span><span class="kw">ifelse</span>(earlystopping.list<span class="op">$</span><span class="kw">predict</span>(x.test) <span class="op">&gt;</span><span class="st"> </span><span class="fl">0.5</span>, <span class="dv">1</span>, <span class="dv">0</span>)
    L2.predict &lt;-<span class="st"> </span><span class="kw">ifelse</span>(L2.list<span class="op">$</span><span class="kw">predict</span>(x.test) <span class="op">&gt;</span><span class="st"> </span><span class="fl">0.5</span>, <span class="dv">1</span>, <span class="dv">0</span>)
    baseline.predict &lt;-<span class="st"> </span><span class="kw">ifelse</span>(<span class="kw">mean</span>(y.test) <span class="op">&gt;</span><span class="st"> </span><span class="fl">0.5</span> , <span class="dv">1</span>, <span class="dv">0</span>)
    <span class="co"># baseline.predict &lt;- mean(y.test)</span>

  } <span class="cf">else</span>{
    <span class="co"># regression data</span>
    earlystopping.list &lt;-
<span class="st">      </span><span class="kw">LMSquareLossEarlyStoppingCV</span>(x.train, y.train, <span class="ot">NULL</span>, 50L)
    L2.list &lt;-<span class="st"> </span><span class="kw">LMSquareLossL2CV</span>(x.train, y.train, <span class="ot">NULL</span>, penalty.vec)

    earlystopping.predict &lt;-<span class="st"> </span>earlystopping.list<span class="op">$</span><span class="kw">predict</span>(x.test)
    L2.predict &lt;-<span class="st"> </span>L2.list<span class="op">$</span><span class="kw">predict</span>(x.test)
    baseline.predict &lt;-<span class="st"> </span><span class="kw">mean</span>(y.test)
  }

  <span class="co"># L2 loss</span>
  earlystopping.loss &lt;-<span class="st"> </span><span class="kw">mean</span>((earlystopping.predict <span class="op">-</span><span class="st"> </span>y.test) <span class="op">^</span><span class="st"> </span><span class="dv">2</span>)
  L2.loss &lt;-<span class="st"> </span><span class="kw">mean</span>((L2.predict <span class="op">-</span><span class="st"> </span>y.test) <span class="op">^</span><span class="st"> </span><span class="dv">2</span>)
  baseline.loss &lt;-<span class="st"> </span><span class="kw">mean</span>((baseline.predict <span class="op">-</span><span class="st"> </span>y.test) <span class="op">^</span><span class="st"> </span><span class="dv">2</span>)

  test.loss.mat[i.fold,] =<span class="st"> </span><span class="kw">c</span>(earlystopping.loss, L2.loss, baseline.loss)
}</code></pre></div>
<div id="matrix-of-loss-values" class="section level3">
<h3>Matrix of loss values</h3>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># show result</span>
<span class="kw">colnames</span>(test.loss.mat) &lt;-<span class="st"> </span><span class="kw">c</span>(<span class="st">&quot;Early Stopping&quot;</span>, <span class="st">&quot;L2&quot;</span>, <span class="st">&quot;Baseline&quot;</span>)

test.loss.mat
<span class="co">#&gt;      Early Stopping        L2  Baseline</span>
<span class="co">#&gt; [1,]     0.10425717 0.1476977 0.4144222</span>
<span class="co">#&gt; [2,]     0.09043478 0.1095652 0.3991304</span>
<span class="co">#&gt; [3,]     0.10695652 0.1043478 0.3878261</span>
<span class="co">#&gt; [4,]     0.10086957 0.1104348 0.3747826</span>

<span class="co"># plot result</span>
<span class="kw">barplot</span>(
  test.loss.mat,
  <span class="dt">main =</span> <span class="kw">c</span>(<span class="st">&quot;Binary Classification: &quot;</span>, <span class="st">&quot;spam&quot;</span>),
  <span class="dt">xlab =</span> <span class="st">&quot;mean loss value&quot;</span>,
  <span class="dt">legend =</span> (<span class="kw">rownames</span>(test.loss.mat)),
  <span class="dt">beside =</span> <span class="ot">TRUE</span>
)</code></pre></div>
<p><img src="data:image/png;base64,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" /><!-- --></p>
<p>Comment on difference in accuracy:</p>
</div>
<div id="trainvalidation-loss-plot" class="section level3">
<h3>Train/validation loss plot</h3>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># Run CV for whole dataset</span>
<span class="cf">if</span>(data.set<span class="op">$</span>is.<span class="dv">01</span>){
  <span class="co"># Binary</span>
  model.list &lt;-<span class="st"> </span><span class="kw">LMLogisticLossL2CV</span>(data.set<span class="op">$</span>features, data.set<span class="op">$</span>labels, <span class="ot">NULL</span>, penalty.vec)
}<span class="cf">else</span>{
  <span class="co"># Regression</span>
  model.list &lt;-<span class="st"> </span><span class="kw">LMSquareLossL2CV</span>(data.set<span class="op">$</span>features,data.set<span class="op">$</span>labels,<span class="ot">NULL</span>, penalty.vec)
}

dot.x &lt;-<span class="st"> </span>model.list<span class="op">$</span>selected.penalty
dot.y &lt;-<span class="st"> </span>model.list<span class="op">$</span>mean.validation.loss.vec[penalty.vec <span class="op">==</span><span class="st"> </span>model.list<span class="op">$</span>selected.penalty]

<span class="kw">matplot</span>(
  <span class="dt">y =</span> <span class="kw">cbind</span>(model.list<span class="op">$</span>mean.validation.loss.vec, model.list<span class="op">$</span>mean.train.loss.vec),
  <span class="dt">x =</span> <span class="kw">as.matrix</span>(penalty.vec),
  <span class="dt">xlab =</span> <span class="st">&quot;penalty&quot;</span>,
  <span class="dt">ylab =</span> <span class="st">&quot;mean loss value&quot;</span>,
  <span class="dt">type =</span> <span class="st">&quot;l&quot;</span>,
  <span class="dt">lty =</span> <span class="dv">1</span><span class="op">:</span><span class="dv">2</span>,
  <span class="dt">pch =</span> <span class="dv">15</span>,
  <span class="dt">col =</span> <span class="kw">c</span>(<span class="dv">17</span>)
)

<span class="kw">matpoints</span>(<span class="dt">x =</span> dot.x,
          <span class="dt">y =</span> dot.y,
          <span class="dt">col =</span> <span class="dv">2</span>,
          <span class="dt">pch =</span> <span class="dv">19</span>)
<span class="kw">legend</span>(
  <span class="dt">x =</span> <span class="kw">length</span>(penalty.vec),
  <span class="dv">0</span>,
  <span class="kw">c</span>(<span class="st">&quot;Validation loss&quot;</span>, <span class="st">&quot;Train loss&quot;</span>),
  <span class="dt">lty =</span> <span class="dv">1</span><span class="op">:</span><span class="dv">2</span>,
  <span class="dt">xjust =</span> <span class="dv">1</span>,
  <span class="dt">yjust =</span> <span class="dv">0</span>
)</code></pre></div>
<p><img src="data:image/png;base64,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" /><!-- --></p>
<p>The optimal number of neighbors is:</p>
<p>Compare against NNLearnCV, and comment on whether or not linear models or nearest neighbors is more accurate:</p>
</div>
</div>
<div id="data-set-2-saheart" class="section level2">
<h2>Data set 2: SAheart</h2>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co">#SAheart</span>
data.name  =<span class="st"> </span><span class="dv">2</span>
data.set &lt;-<span class="st"> </span>data.list[[data.name]]
test.loss.mat &lt;-<span class="st"> </span><span class="kw">matrix</span>(<span class="dv">0</span>, <span class="dt">nrow =</span> <span class="dv">4</span>, <span class="dt">ncol =</span> <span class="dv">3</span>)

<span class="co">#Check data type here:</span>
<span class="kw">set.seed</span>(<span class="dv">2</span>)

fold.vec &lt;-<span class="st"> </span><span class="kw">sample</span>(<span class="kw">rep</span>(<span class="dv">1</span><span class="op">:</span>n.folds, <span class="dt">l =</span> <span class="kw">length</span>(data.set<span class="op">$</span>labels)))

penalty.vec &lt;-<span class="st"> </span><span class="kw">seq</span>(<span class="dv">5</span>, <span class="fl">0.1</span>, <span class="dt">by =</span> <span class="op">-</span><span class="fl">0.1</span>)

<span class="cf">for</span> (i.fold <span class="cf">in</span> (<span class="dv">1</span><span class="op">:</span>n.folds)) {
  train.index &lt;-<span class="st"> </span>fold.vec <span class="op">!=</span><span class="st"> </span>i.fold
  test.index &lt;-<span class="st"> </span><span class="op">!</span>train.index

  x.train &lt;-<span class="st"> </span>data.set<span class="op">$</span>features[train.index, ]
  y.train &lt;-<span class="st"> </span>data.set<span class="op">$</span>labels[train.index]
  x.test &lt;-<span class="st"> </span>data.set<span class="op">$</span>feature[test.index, ]
  y.test &lt;-<span class="st"> </span>data.set<span class="op">$</span>labels[test.index]

  <span class="cf">if</span> (data.set<span class="op">$</span>is.<span class="dv">01</span>) {
    <span class="co"># binary data</span>
    earlystopping.list &lt;-
<span class="st">      </span><span class="kw">LMLogisticLossEarlyStoppingCV</span>(x.train, y.train, <span class="ot">NULL</span>, 100L, <span class="fl">0.5</span>)
    L2.list &lt;-
<span class="st">      </span><span class="kw">LMLogisticLossL2CV</span>(x.train, y.train, <span class="ot">NULL</span>, penalty.vec)

    earlystopping.predict &lt;-
<span class="st">      </span><span class="kw">ifelse</span>(earlystopping.list<span class="op">$</span><span class="kw">predict</span>(x.test) <span class="op">&gt;</span><span class="st"> </span><span class="fl">0.5</span>, <span class="dv">1</span>, <span class="dv">0</span>)
    L2.predict &lt;-<span class="st"> </span><span class="kw">ifelse</span>(L2.list<span class="op">$</span><span class="kw">predict</span>(x.test) <span class="op">&gt;</span><span class="st"> </span><span class="fl">0.5</span>, <span class="dv">1</span>, <span class="dv">0</span>)
    baseline.predict &lt;-<span class="st"> </span><span class="kw">ifelse</span>(<span class="kw">mean</span>(y.test) <span class="op">&gt;</span><span class="st"> </span><span class="fl">0.5</span> , <span class="dv">1</span>, <span class="dv">0</span>)
    <span class="co"># baseline.predict &lt;- mean(y.test)</span>

  } <span class="cf">else</span>{
    <span class="co"># regression data</span>
    earlystopping.list &lt;-
<span class="st">      </span><span class="kw">LMSquareLossEarlyStoppingCV</span>(x.train, y.train, <span class="ot">NULL</span>, 50L)
    L2.list &lt;-<span class="st"> </span><span class="kw">LMSquareLossL2CV</span>(x.train, y.train, <span class="ot">NULL</span>, penalty.vec)

    earlystopping.predict &lt;-<span class="st"> </span>earlystopping.list<span class="op">$</span><span class="kw">predict</span>(x.test)
    L2.predict &lt;-<span class="st"> </span>L2.list<span class="op">$</span><span class="kw">predict</span>(x.test)
    baseline.predict &lt;-<span class="st"> </span><span class="kw">mean</span>(y.test)
  }

  <span class="co"># L2 loss</span>
  earlystopping.loss &lt;-<span class="st"> </span><span class="kw">mean</span>((earlystopping.predict <span class="op">-</span><span class="st"> </span>y.test) <span class="op">^</span><span class="st"> </span><span class="dv">2</span>)
  L2.loss &lt;-<span class="st"> </span><span class="kw">mean</span>((L2.predict <span class="op">-</span><span class="st"> </span>y.test) <span class="op">^</span><span class="st"> </span><span class="dv">2</span>)
  baseline.loss &lt;-<span class="st"> </span><span class="kw">mean</span>((baseline.predict <span class="op">-</span><span class="st"> </span>y.test) <span class="op">^</span><span class="st"> </span><span class="dv">2</span>)

  test.loss.mat[i.fold,] =<span class="st"> </span><span class="kw">c</span>(earlystopping.loss, L2.loss, baseline.loss)
}</code></pre></div>
<div id="matrix-of-loss-values-1" class="section level3">
<h3>Matrix of loss values</h3>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># show result</span>
<span class="kw">colnames</span>(test.loss.mat) &lt;-<span class="st"> </span><span class="kw">c</span>(<span class="st">&quot;Early Stopping&quot;</span>, <span class="st">&quot;L2&quot;</span>, <span class="st">&quot;Baseline&quot;</span>)

test.loss.mat
<span class="co">#&gt;      Early Stopping        L2  Baseline</span>
<span class="co">#&gt; [1,]      0.3620690 0.3534483 0.4137931</span>
<span class="co">#&gt; [2,]      0.2931034 0.3017241 0.3534483</span>
<span class="co">#&gt; [3,]      0.3130435 0.3217391 0.2608696</span>
<span class="co">#&gt; [4,]      0.3130435 0.2956522 0.3565217</span>

<span class="co"># plot result</span>
<span class="kw">barplot</span>(
  test.loss.mat,
  <span class="dt">main =</span> <span class="kw">c</span>(<span class="st">&quot;Binary Classification: &quot;</span>, <span class="st">&quot;SAheart&quot;</span>),
  <span class="dt">xlab =</span> <span class="st">&quot;mean loss value&quot;</span>,
  <span class="dt">legend =</span> (<span class="kw">rownames</span>(test.loss.mat)),
  <span class="dt">beside =</span> <span class="ot">TRUE</span>
)</code></pre></div>
<p><img src="data:image/png;base64,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" /><!-- --></p>
<p>Comment on difference in accuracy:</p>
</div>
<div id="trainvalidation-loss-plot-1" class="section level3">
<h3>Train/validation loss plot</h3>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># Run CV for whole dataset</span>
<span class="cf">if</span>(data.set<span class="op">$</span>is.<span class="dv">01</span>){
  <span class="co"># Binary</span>
  model.list &lt;-<span class="st"> </span><span class="kw">LMLogisticLossL2CV</span>(data.set<span class="op">$</span>features, data.set<span class="op">$</span>labels, <span class="ot">NULL</span>, penalty.vec)
}<span class="cf">else</span>{
  <span class="co"># Regression</span>
  model.list &lt;-<span class="st"> </span><span class="kw">LMSquareLossL2CV</span>(data.set<span class="op">$</span>features,data.set<span class="op">$</span>labels,<span class="ot">NULL</span>, penalty.vec)
}

dot.x &lt;-<span class="st"> </span>model.list<span class="op">$</span>selected.penalty
dot.y &lt;-<span class="st"> </span>model.list<span class="op">$</span>mean.validation.loss.vec[penalty.vec <span class="op">==</span><span class="st"> </span>model.list<span class="op">$</span>selected.penalty]

<span class="kw">matplot</span>(
  <span class="dt">y =</span> <span class="kw">cbind</span>(model.list<span class="op">$</span>mean.validation.loss.vec, model.list<span class="op">$</span>mean.train.loss.vec),
  <span class="dt">x =</span> <span class="kw">as.matrix</span>(penalty.vec),
  <span class="dt">xlab =</span> <span class="st">&quot;penalty&quot;</span>,
  <span class="dt">ylab =</span> <span class="st">&quot;mean loss value&quot;</span>,
  <span class="dt">type =</span> <span class="st">&quot;l&quot;</span>,
  <span class="dt">lty =</span> <span class="dv">1</span><span class="op">:</span><span class="dv">2</span>,
  <span class="dt">pch =</span> <span class="dv">15</span>,
  <span class="dt">col =</span> <span class="kw">c</span>(<span class="dv">17</span>)
)

<span class="kw">matpoints</span>(<span class="dt">x =</span> dot.x,
          <span class="dt">y =</span> dot.y,
          <span class="dt">col =</span> <span class="dv">2</span>,
          <span class="dt">pch =</span> <span class="dv">19</span>)
<span class="kw">legend</span>(
  <span class="dt">x =</span> <span class="kw">length</span>(penalty.vec),
  <span class="dv">0</span>,
  <span class="kw">c</span>(<span class="st">&quot;Validation loss&quot;</span>, <span class="st">&quot;Train loss&quot;</span>),
  <span class="dt">lty =</span> <span class="dv">1</span><span class="op">:</span><span class="dv">2</span>,
  <span class="dt">xjust =</span> <span class="dv">1</span>,
  <span class="dt">yjust =</span> <span class="dv">0</span>
)</code></pre></div>
<p><img src="data:image/png;base64,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" /><!-- --></p>
<p>The optimal number of neighbors is:</p>
<p>Compare against NNLearnCV, and comment on whether or not linear models or nearest neighbors is more accurate:</p>
</div>
</div>
<div id="data-set-3-zip.train" class="section level2">
<h2>Data set 3: zip.train</h2>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co">#zip.train</span>
data.name  =<span class="st"> </span><span class="dv">3</span>
data.set &lt;-<span class="st"> </span>data.list[[data.name]]
test.loss.mat &lt;-<span class="st"> </span><span class="kw">matrix</span>(<span class="dv">0</span>, <span class="dt">nrow =</span> <span class="dv">4</span>, <span class="dt">ncol =</span> <span class="dv">3</span>)

<span class="co">#Check data type here:</span>
<span class="kw">set.seed</span>(<span class="dv">2</span>)

fold.vec &lt;-<span class="st"> </span><span class="kw">sample</span>(<span class="kw">rep</span>(<span class="dv">1</span><span class="op">:</span>n.folds, <span class="dt">l =</span> <span class="kw">length</span>(data.set<span class="op">$</span>labels)))

penalty.vec &lt;-<span class="st"> </span><span class="kw">seq</span>(<span class="dv">5</span>, <span class="fl">0.1</span>, <span class="dt">by =</span> <span class="op">-</span><span class="fl">0.1</span>)

<span class="cf">for</span> (i.fold <span class="cf">in</span> (<span class="dv">1</span><span class="op">:</span>n.folds)) {
  train.index &lt;-<span class="st"> </span>fold.vec <span class="op">!=</span><span class="st"> </span>i.fold
  test.index &lt;-<span class="st"> </span><span class="op">!</span>train.index

  x.train &lt;-<span class="st"> </span>data.set<span class="op">$</span>features[train.index, ]
  y.train &lt;-<span class="st"> </span>data.set<span class="op">$</span>labels[train.index]
  x.test &lt;-<span class="st"> </span>data.set<span class="op">$</span>feature[test.index, ]
  y.test &lt;-<span class="st"> </span>data.set<span class="op">$</span>labels[test.index]

  <span class="cf">if</span> (data.set<span class="op">$</span>is.<span class="dv">01</span>) {
    <span class="co"># binary data</span>
    earlystopping.list &lt;-
<span class="st">      </span><span class="kw">LMLogisticLossEarlyStoppingCV</span>(x.train, y.train, <span class="ot">NULL</span>, 100L, <span class="fl">0.5</span>)
    L2.list &lt;-
<span class="st">      </span><span class="kw">LMLogisticLossL2CV</span>(x.train, y.train, <span class="ot">NULL</span>, penalty.vec)

    earlystopping.predict &lt;-
<span class="st">      </span><span class="kw">ifelse</span>(earlystopping.list<span class="op">$</span><span class="kw">predict</span>(x.test) <span class="op">&gt;</span><span class="st"> </span><span class="fl">0.5</span>, <span class="dv">1</span>, <span class="dv">0</span>)
    L2.predict &lt;-<span class="st"> </span><span class="kw">ifelse</span>(L2.list<span class="op">$</span><span class="kw">predict</span>(x.test) <span class="op">&gt;</span><span class="st"> </span><span class="fl">0.5</span>, <span class="dv">1</span>, <span class="dv">0</span>)
    baseline.predict &lt;-<span class="st"> </span><span class="kw">ifelse</span>(<span class="kw">mean</span>(y.test) <span class="op">&gt;</span><span class="st"> </span><span class="fl">0.5</span> , <span class="dv">1</span>, <span class="dv">0</span>)
    <span class="co"># baseline.predict &lt;- mean(y.test)</span>

  } <span class="cf">else</span>{
    <span class="co"># regression data</span>
    earlystopping.list &lt;-
<span class="st">      </span><span class="kw">LMSquareLossEarlyStoppingCV</span>(x.train, y.train, <span class="ot">NULL</span>, 50L)
    L2.list &lt;-<span class="st"> </span><span class="kw">LMSquareLossL2CV</span>(x.train, y.train, <span class="ot">NULL</span>, penalty.vec)

    earlystopping.predict &lt;-<span class="st"> </span>earlystopping.list<span class="op">$</span><span class="kw">predict</span>(x.test)
    L2.predict &lt;-<span class="st"> </span>L2.list<span class="op">$</span><span class="kw">predict</span>(x.test)
    baseline.predict &lt;-<span class="st"> </span><span class="kw">mean</span>(y.test)
  }

  <span class="co"># L2 loss</span>
  earlystopping.loss &lt;-<span class="st"> </span><span class="kw">mean</span>((earlystopping.predict <span class="op">-</span><span class="st"> </span>y.test) <span class="op">^</span><span class="st"> </span><span class="dv">2</span>)
  L2.loss &lt;-<span class="st"> </span><span class="kw">mean</span>((L2.predict <span class="op">-</span><span class="st"> </span>y.test) <span class="op">^</span><span class="st"> </span><span class="dv">2</span>)
  baseline.loss &lt;-<span class="st"> </span><span class="kw">mean</span>((baseline.predict <span class="op">-</span><span class="st"> </span>y.test) <span class="op">^</span><span class="st"> </span><span class="dv">2</span>)

  test.loss.mat[i.fold,] =<span class="st"> </span><span class="kw">c</span>(earlystopping.loss, L2.loss, baseline.loss)
}</code></pre></div>
<div id="matrix-of-loss-values-2" class="section level3">
<h3>Matrix of loss values</h3>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># show result</span>
<span class="kw">colnames</span>(test.loss.mat) &lt;-<span class="st"> </span><span class="kw">c</span>(<span class="st">&quot;Early Stopping&quot;</span>, <span class="st">&quot;L2&quot;</span>, <span class="st">&quot;Baseline&quot;</span>)

test.loss.mat
<span class="co">#&gt;      Early Stopping L2  Baseline</span>
<span class="co">#&gt; [1,]    0.000000000  0 0.4709091</span>
<span class="co">#&gt; [2,]    0.000000000  0 0.4309091</span>
<span class="co">#&gt; [3,]    0.001818182  0 0.4818182</span>
<span class="co">#&gt; [4,]    0.001821494  0 0.4080146</span>

<span class="co"># plot result</span>
<span class="kw">barplot</span>(
  test.loss.mat,
  <span class="dt">main =</span> <span class="kw">c</span>(<span class="st">&quot;Binary Classification: &quot;</span>, <span class="st">&quot;zip.train&quot;</span>),
  <span class="dt">xlab =</span> <span class="st">&quot;mean loss value&quot;</span>,
  <span class="dt">legend =</span> (<span class="kw">rownames</span>(test.loss.mat)),
  <span class="dt">beside =</span> <span class="ot">TRUE</span>
)</code></pre></div>
<p><img src="data:image/png;base64,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" /><!-- --></p>
<p>Comment on difference in accuracy:</p>
</div>
<div id="trainvalidation-loss-plot-2" class="section level3">
<h3>Train/validation loss plot</h3>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># Run CV for whole dataset</span>
<span class="cf">if</span>(data.set<span class="op">$</span>is.<span class="dv">01</span>){
  <span class="co"># Binary</span>
  model.list &lt;-<span class="st"> </span><span class="kw">LMLogisticLossL2CV</span>(data.set<span class="op">$</span>features, data.set<span class="op">$</span>labels, <span class="ot">NULL</span>, penalty.vec)
}<span class="cf">else</span>{
  <span class="co"># Regression</span>
  model.list &lt;-<span class="st"> </span><span class="kw">LMSquareLossL2CV</span>(data.set<span class="op">$</span>features,data.set<span class="op">$</span>labels,<span class="ot">NULL</span>, penalty.vec)
}

dot.x &lt;-<span class="st"> </span>model.list<span class="op">$</span>selected.penalty
dot.y &lt;-<span class="st"> </span>model.list<span class="op">$</span>mean.validation.loss.vec[penalty.vec <span class="op">==</span><span class="st"> </span>model.list<span class="op">$</span>selected.penalty]

<span class="kw">matplot</span>(
  <span class="dt">y =</span> <span class="kw">cbind</span>(model.list<span class="op">$</span>mean.validation.loss.vec, model.list<span class="op">$</span>mean.train.loss.vec),
  <span class="dt">x =</span> <span class="kw">as.matrix</span>(penalty.vec),
  <span class="dt">xlab =</span> <span class="st">&quot;penalty&quot;</span>,
  <span class="dt">ylab =</span> <span class="st">&quot;mean loss value&quot;</span>,
  <span class="dt">type =</span> <span class="st">&quot;l&quot;</span>,
  <span class="dt">lty =</span> <span class="dv">1</span><span class="op">:</span><span class="dv">2</span>,
  <span class="dt">pch =</span> <span class="dv">15</span>,
  <span class="dt">col =</span> <span class="kw">c</span>(<span class="dv">17</span>)
)

<span class="kw">matpoints</span>(<span class="dt">x =</span> dot.x,
          <span class="dt">y =</span> dot.y,
          <span class="dt">col =</span> <span class="dv">2</span>,
          <span class="dt">pch =</span> <span class="dv">19</span>)
<span class="kw">legend</span>(
  <span class="dt">x =</span> <span class="kw">length</span>(penalty.vec),
  <span class="dv">0</span>,
  <span class="kw">c</span>(<span class="st">&quot;Validation loss&quot;</span>, <span class="st">&quot;Train loss&quot;</span>),
  <span class="dt">lty =</span> <span class="dv">1</span><span class="op">:</span><span class="dv">2</span>,
  <span class="dt">xjust =</span> <span class="dv">1</span>,
  <span class="dt">yjust =</span> <span class="dv">0</span>
)</code></pre></div>
<p><img src="data:image/png;base64,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" /><!-- --></p>
<p>The optimal number of neighbors is:</p>
<p>Compare against NNLearnCV, and comment on whether or not linear models or nearest neighbors is more accurate:</p>
</div>
</div>
<div id="data-set-4-prostate" class="section level2">
<h2>Data set 4: prostate</h2>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co">#prostate</span>
data.name  =<span class="st"> </span><span class="dv">4</span>
data.set &lt;-<span class="st"> </span>data.list[[data.name]]
test.loss.mat &lt;-<span class="st"> </span><span class="kw">matrix</span>(<span class="dv">0</span>, <span class="dt">nrow =</span> <span class="dv">4</span>, <span class="dt">ncol =</span> <span class="dv">3</span>)

<span class="co">#Check data type here:</span>
<span class="kw">set.seed</span>(<span class="dv">2</span>)

fold.vec &lt;-<span class="st"> </span><span class="kw">sample</span>(<span class="kw">rep</span>(<span class="dv">1</span><span class="op">:</span>n.folds, <span class="dt">l =</span> <span class="kw">length</span>(data.set<span class="op">$</span>labels)))

penalty.vec &lt;-<span class="st"> </span><span class="kw">seq</span>(<span class="dv">5</span>, <span class="fl">0.1</span>, <span class="dt">by =</span> <span class="op">-</span><span class="fl">0.1</span>)

<span class="cf">for</span> (i.fold <span class="cf">in</span> (<span class="dv">1</span><span class="op">:</span>n.folds)) {
  train.index &lt;-<span class="st"> </span>fold.vec <span class="op">!=</span><span class="st"> </span>i.fold
  test.index &lt;-<span class="st"> </span><span class="op">!</span>train.index

  x.train &lt;-<span class="st"> </span>data.set<span class="op">$</span>features[train.index, ]
  y.train &lt;-<span class="st"> </span>data.set<span class="op">$</span>labels[train.index]
  x.test &lt;-<span class="st"> </span>data.set<span class="op">$</span>feature[test.index, ]
  y.test &lt;-<span class="st"> </span>data.set<span class="op">$</span>labels[test.index]

  <span class="cf">if</span> (data.set<span class="op">$</span>is.<span class="dv">01</span>) {
    <span class="co"># binary data</span>
    earlystopping.list &lt;-
<span class="st">      </span><span class="kw">LMLogisticLossEarlyStoppingCV</span>(x.train, y.train, <span class="ot">NULL</span>, 100L, <span class="fl">0.5</span>)
    L2.list &lt;-
<span class="st">      </span><span class="kw">LMLogisticLossL2CV</span>(x.train, y.train, <span class="ot">NULL</span>, penalty.vec)

    earlystopping.predict &lt;-
<span class="st">      </span><span class="kw">ifelse</span>(earlystopping.list<span class="op">$</span><span class="kw">predict</span>(x.test) <span class="op">&gt;</span><span class="st"> </span><span class="fl">0.5</span>, <span class="dv">1</span>, <span class="dv">0</span>)
    L2.predict &lt;-<span class="st"> </span><span class="kw">ifelse</span>(L2.list<span class="op">$</span><span class="kw">predict</span>(x.test) <span class="op">&gt;</span><span class="st"> </span><span class="fl">0.5</span>, <span class="dv">1</span>, <span class="dv">0</span>)
    baseline.predict &lt;-<span class="st"> </span><span class="kw">ifelse</span>(<span class="kw">mean</span>(y.test) <span class="op">&gt;</span><span class="st"> </span><span class="fl">0.5</span> , <span class="dv">1</span>, <span class="dv">0</span>)
    <span class="co"># baseline.predict &lt;- mean(y.test)</span>

  } <span class="cf">else</span>{
    <span class="co"># regression data</span>
    earlystopping.list &lt;-
<span class="st">      </span><span class="kw">LMSquareLossEarlyStoppingCV</span>(x.train, y.train, <span class="ot">NULL</span>, 50L)
    L2.list &lt;-<span class="st"> </span><span class="kw">LMSquareLossL2CV</span>(x.train, y.train, <span class="ot">NULL</span>, penalty.vec)

    earlystopping.predict &lt;-<span class="st"> </span>earlystopping.list<span class="op">$</span><span class="kw">predict</span>(x.test)
    L2.predict &lt;-<span class="st"> </span>L2.list<span class="op">$</span><span class="kw">predict</span>(x.test)
    baseline.predict &lt;-<span class="st"> </span><span class="kw">mean</span>(y.test)
  }

  <span class="co"># L2 loss</span>
  earlystopping.loss &lt;-<span class="st"> </span><span class="kw">mean</span>((earlystopping.predict <span class="op">-</span><span class="st"> </span>y.test) <span class="op">^</span><span class="st"> </span><span class="dv">2</span>)
  L2.loss &lt;-<span class="st"> </span><span class="kw">mean</span>((L2.predict <span class="op">-</span><span class="st"> </span>y.test) <span class="op">^</span><span class="st"> </span><span class="dv">2</span>)
  baseline.loss &lt;-<span class="st"> </span><span class="kw">mean</span>((baseline.predict <span class="op">-</span><span class="st"> </span>y.test) <span class="op">^</span><span class="st"> </span><span class="dv">2</span>)

  test.loss.mat[i.fold,] =<span class="st"> </span><span class="kw">c</span>(earlystopping.loss, L2.loss, baseline.loss)
}</code></pre></div>
<div id="matrix-of-loss-values-3" class="section level3">
<h3>Matrix of loss values</h3>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># show result</span>
<span class="kw">colnames</span>(test.loss.mat) &lt;-<span class="st"> </span><span class="kw">c</span>(<span class="st">&quot;Early Stopping&quot;</span>, <span class="st">&quot;L2&quot;</span>, <span class="st">&quot;Baseline&quot;</span>)

test.loss.mat
<span class="co">#&gt;      Early Stopping        L2  Baseline</span>
<span class="co">#&gt; [1,]       4.350123 0.3829302 0.7514432</span>
<span class="co">#&gt; [2,]       6.566584 0.4981539 1.1907989</span>
<span class="co">#&gt; [3,]       3.511827 0.8962265 1.4294445</span>
<span class="co">#&gt; [4,]       3.019656 0.6639192 1.9157589</span>

<span class="co"># plot result</span>
<span class="kw">barplot</span>(
  test.loss.mat,
  <span class="dt">main =</span> <span class="kw">c</span>(<span class="st">&quot;Binary Classification: &quot;</span>, <span class="st">&quot;prostate&quot;</span>),
  <span class="dt">xlab =</span> <span class="st">&quot;mean loss value&quot;</span>,
  <span class="dt">legend =</span> (<span class="kw">rownames</span>(test.loss.mat)),
  <span class="dt">beside =</span> <span class="ot">TRUE</span>
)</code></pre></div>
<p><img src="data:image/png;base64,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" /><!-- --></p>
<p>Comment on difference in accuracy:</p>
</div>
<div id="trainvalidation-loss-plot-3" class="section level3">
<h3>Train/validation loss plot</h3>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># Run CV for whole dataset</span>
<span class="cf">if</span>(data.set<span class="op">$</span>is.<span class="dv">01</span>){
  <span class="co"># Binary</span>
  model.list &lt;-<span class="st"> </span><span class="kw">LMLogisticLossL2CV</span>(data.set<span class="op">$</span>features, data.set<span class="op">$</span>labels, <span class="ot">NULL</span>, penalty.vec)
}<span class="cf">else</span>{
  <span class="co"># Regression</span>
  model.list &lt;-<span class="st"> </span><span class="kw">LMSquareLossL2CV</span>(data.set<span class="op">$</span>features,data.set<span class="op">$</span>labels,<span class="ot">NULL</span>, penalty.vec)
}

dot.x &lt;-<span class="st"> </span>model.list<span class="op">$</span>selected.penalty
dot.y &lt;-<span class="st"> </span>model.list<span class="op">$</span>mean.validation.loss.vec[penalty.vec <span class="op">==</span><span class="st"> </span>model.list<span class="op">$</span>selected.penalty]

<span class="kw">matplot</span>(
  <span class="dt">y =</span> <span class="kw">cbind</span>(model.list<span class="op">$</span>mean.validation.loss.vec, model.list<span class="op">$</span>mean.train.loss.vec),
  <span class="dt">x =</span> <span class="kw">as.matrix</span>(penalty.vec),
  <span class="dt">xlab =</span> <span class="st">&quot;penalty&quot;</span>,
  <span class="dt">ylab =</span> <span class="st">&quot;mean loss value&quot;</span>,
  <span class="dt">type =</span> <span class="st">&quot;l&quot;</span>,
  <span class="dt">lty =</span> <span class="dv">1</span><span class="op">:</span><span class="dv">2</span>,
  <span class="dt">pch =</span> <span class="dv">15</span>,
  <span class="dt">col =</span> <span class="kw">c</span>(<span class="dv">17</span>)
)

<span class="kw">matpoints</span>(<span class="dt">x =</span> dot.x,
          <span class="dt">y =</span> dot.y,
          <span class="dt">col =</span> <span class="dv">2</span>,
          <span class="dt">pch =</span> <span class="dv">19</span>)
<span class="kw">legend</span>(
  <span class="dt">x =</span> <span class="kw">length</span>(penalty.vec),
  <span class="dv">0</span>,
  <span class="kw">c</span>(<span class="st">&quot;Validation loss&quot;</span>, <span class="st">&quot;Train loss&quot;</span>),
  <span class="dt">lty =</span> <span class="dv">1</span><span class="op">:</span><span class="dv">2</span>,
  <span class="dt">xjust =</span> <span class="dv">1</span>,
  <span class="dt">yjust =</span> <span class="dv">0</span>
)</code></pre></div>
<p><img src="data:image/png;base64,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" /><!-- --></p>
<p>The optimal number of neighbors is:</p>
<p>Compare against NNLearnCV, and comment on whether or not linear models or nearest neighbors is more accurate:</p>
</div>
</div>
<div id="data-set-5-ozone" class="section level2">
<h2>Data set 5: ozone</h2>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co">#ozone</span>
data.name  =<span class="st"> </span><span class="dv">5</span>
data.set &lt;-<span class="st"> </span>data.list[[data.name]]
test.loss.mat &lt;-<span class="st"> </span><span class="kw">matrix</span>(<span class="dv">0</span>, <span class="dt">nrow =</span> <span class="dv">4</span>, <span class="dt">ncol =</span> <span class="dv">3</span>)

<span class="co">#Check data type here:</span>
<span class="kw">set.seed</span>(<span class="dv">2</span>)

fold.vec &lt;-<span class="st"> </span><span class="kw">sample</span>(<span class="kw">rep</span>(<span class="dv">1</span><span class="op">:</span>n.folds, <span class="dt">l =</span> <span class="kw">length</span>(data.set<span class="op">$</span>labels)))

penalty.vec &lt;-<span class="st"> </span><span class="kw">seq</span>(<span class="dv">5</span>, <span class="fl">0.1</span>, <span class="dt">by =</span> <span class="op">-</span><span class="fl">0.1</span>)

<span class="cf">for</span> (i.fold <span class="cf">in</span> (<span class="dv">1</span><span class="op">:</span>n.folds)) {
  train.index &lt;-<span class="st"> </span>fold.vec <span class="op">!=</span><span class="st"> </span>i.fold
  test.index &lt;-<span class="st"> </span><span class="op">!</span>train.index

  x.train &lt;-<span class="st"> </span>data.set<span class="op">$</span>features[train.index, ]
  y.train &lt;-<span class="st"> </span>data.set<span class="op">$</span>labels[train.index]
  x.test &lt;-<span class="st"> </span>data.set<span class="op">$</span>feature[test.index, ]
  y.test &lt;-<span class="st"> </span>data.set<span class="op">$</span>labels[test.index]

  <span class="cf">if</span> (data.set<span class="op">$</span>is.<span class="dv">01</span>) {
    <span class="co"># binary data</span>
    earlystopping.list &lt;-
<span class="st">      </span><span class="kw">LMLogisticLossEarlyStoppingCV</span>(x.train, y.train, <span class="ot">NULL</span>, 100L, <span class="fl">0.5</span>)
    L2.list &lt;-
<span class="st">      </span><span class="kw">LMLogisticLossL2CV</span>(x.train, y.train, <span class="ot">NULL</span>, penalty.vec)

    earlystopping.predict &lt;-
<span class="st">      </span><span class="kw">ifelse</span>(earlystopping.list<span class="op">$</span><span class="kw">predict</span>(x.test) <span class="op">&gt;</span><span class="st"> </span><span class="fl">0.5</span>, <span class="dv">1</span>, <span class="dv">0</span>)
    L2.predict &lt;-<span class="st"> </span><span class="kw">ifelse</span>(L2.list<span class="op">$</span><span class="kw">predict</span>(x.test) <span class="op">&gt;</span><span class="st"> </span><span class="fl">0.5</span>, <span class="dv">1</span>, <span class="dv">0</span>)
    baseline.predict &lt;-<span class="st"> </span><span class="kw">ifelse</span>(<span class="kw">mean</span>(y.test) <span class="op">&gt;</span><span class="st"> </span><span class="fl">0.5</span> , <span class="dv">1</span>, <span class="dv">0</span>)
    <span class="co"># baseline.predict &lt;- mean(y.test)</span>

  } <span class="cf">else</span>{
    <span class="co"># regression data</span>
    earlystopping.list &lt;-
<span class="st">      </span><span class="kw">LMSquareLossEarlyStoppingCV</span>(x.train, y.train, <span class="ot">NULL</span>, 50L)
    L2.list &lt;-<span class="st"> </span><span class="kw">LMSquareLossL2CV</span>(x.train, y.train, <span class="ot">NULL</span>, penalty.vec)

    earlystopping.predict &lt;-<span class="st"> </span>earlystopping.list<span class="op">$</span><span class="kw">predict</span>(x.test)
    L2.predict &lt;-<span class="st"> </span>L2.list<span class="op">$</span><span class="kw">predict</span>(x.test)
    baseline.predict &lt;-<span class="st"> </span><span class="kw">mean</span>(y.test)
  }

  <span class="co"># L2 loss</span>
  earlystopping.loss &lt;-<span class="st"> </span><span class="kw">mean</span>((earlystopping.predict <span class="op">-</span><span class="st"> </span>y.test) <span class="op">^</span><span class="st"> </span><span class="dv">2</span>)
  L2.loss &lt;-<span class="st"> </span><span class="kw">mean</span>((L2.predict <span class="op">-</span><span class="st"> </span>y.test) <span class="op">^</span><span class="st"> </span><span class="dv">2</span>)
  baseline.loss &lt;-<span class="st"> </span><span class="kw">mean</span>((baseline.predict <span class="op">-</span><span class="st"> </span>y.test) <span class="op">^</span><span class="st"> </span><span class="dv">2</span>)

  test.loss.mat[i.fold,] =<span class="st"> </span><span class="kw">c</span>(earlystopping.loss, L2.loss, baseline.loss)
}</code></pre></div>
<div id="matrix-of-loss-values-4" class="section level3">
<h3>Matrix of loss values</h3>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># show result</span>
<span class="kw">colnames</span>(test.loss.mat) &lt;-<span class="st"> </span><span class="kw">c</span>(<span class="st">&quot;Early Stopping&quot;</span>, <span class="st">&quot;L2&quot;</span>, <span class="st">&quot;Baseline&quot;</span>)

test.loss.mat
<span class="co">#&gt;      Early Stopping       L2  Baseline</span>
<span class="co">#&gt; [1,]       462.0757 447.2610  572.3520</span>
<span class="co">#&gt; [2,]       455.0624 413.7400  652.1671</span>
<span class="co">#&gt; [3,]       394.0263 461.3085  879.0038</span>
<span class="co">#&gt; [4,]       897.9711 941.0776 1882.6118</span>

<span class="co"># plot result</span>
<span class="kw">barplot</span>(
  test.loss.mat,
  <span class="dt">main =</span> <span class="kw">c</span>(<span class="st">&quot;Binary Classification: &quot;</span>, <span class="st">&quot;ozone&quot;</span>),
  <span class="dt">xlab =</span> <span class="st">&quot;mean loss value&quot;</span>,
  <span class="dt">legend =</span> (<span class="kw">rownames</span>(test.loss.mat)),
  <span class="dt">beside =</span> <span class="ot">TRUE</span>
)</code></pre></div>
<p><img src="data:image/png;base64,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" /><!-- --></p>
<p>Comment on difference in accuracy:</p>
</div>
<div id="trainvalidation-loss-plot-4" class="section level3">
<h3>Train/validation loss plot</h3>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># Run CV for whole dataset</span>
<span class="cf">if</span>(data.set<span class="op">$</span>is.<span class="dv">01</span>){
  <span class="co"># Binary</span>
  model.list &lt;-<span class="st"> </span><span class="kw">LMLogisticLossL2CV</span>(data.set<span class="op">$</span>features, data.set<span class="op">$</span>labels, <span class="ot">NULL</span>, penalty.vec)
}<span class="cf">else</span>{
  <span class="co"># Regression</span>
  model.list &lt;-<span class="st"> </span><span class="kw">LMSquareLossL2CV</span>(data.set<span class="op">$</span>features,data.set<span class="op">$</span>labels,<span class="ot">NULL</span>, penalty.vec)
}

dot.x &lt;-<span class="st"> </span>model.list<span class="op">$</span>selected.penalty
dot.y &lt;-<span class="st"> </span>model.list<span class="op">$</span>mean.validation.loss.vec[penalty.vec <span class="op">==</span><span class="st"> </span>model.list<span class="op">$</span>selected.penalty]

<span class="kw">matplot</span>(
  <span class="dt">y =</span> <span class="kw">cbind</span>(model.list<span class="op">$</span>mean.validation.loss.vec, model.list<span class="op">$</span>mean.train.loss.vec),
  <span class="dt">x =</span> <span class="kw">as.matrix</span>(penalty.vec),
  <span class="dt">xlab =</span> <span class="st">&quot;penalty&quot;</span>,
  <span class="dt">ylab =</span> <span class="st">&quot;mean loss value&quot;</span>,
  <span class="dt">type =</span> <span class="st">&quot;l&quot;</span>,
  <span class="dt">lty =</span> <span class="dv">1</span><span class="op">:</span><span class="dv">2</span>,
  <span class="dt">pch =</span> <span class="dv">15</span>,
  <span class="dt">col =</span> <span class="kw">c</span>(<span class="dv">17</span>)
)

<span class="kw">matpoints</span>(<span class="dt">x =</span> dot.x,
          <span class="dt">y =</span> dot.y,
          <span class="dt">col =</span> <span class="dv">2</span>,
          <span class="dt">pch =</span> <span class="dv">19</span>)
<span class="kw">legend</span>(
  <span class="dt">x =</span> <span class="kw">length</span>(penalty.vec),
  <span class="dv">0</span>,
  <span class="kw">c</span>(<span class="st">&quot;Validation loss&quot;</span>, <span class="st">&quot;Train loss&quot;</span>),
  <span class="dt">lty =</span> <span class="dv">1</span><span class="op">:</span><span class="dv">2</span>,
  <span class="dt">xjust =</span> <span class="dv">1</span>,
  <span class="dt">yjust =</span> <span class="dv">0</span>
)</code></pre></div>
<p><img src="data:image/png;base64,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" /><!-- --></p>
<p>The optimal number of neighbors is:</p>
<p>Compare against NNLearnCV, and comment on whether or not linear models or nearest neighbors is more accurate:</p>
</div>
</div>
<div id="end-of-the-report" class="section level2">
<h2>End of the report</h2>
</div>



<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML";
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>

</body>
</html>
